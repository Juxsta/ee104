{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten,Dropout,Conv2D,LSTM, LeakyReLU, MaxPooling2D\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from os import walk\n",
    "from keras.preprocessing import sequence\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from scipy.io import wavfile\n",
    "import glob\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from os.path import basename\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "pool=ThreadPool(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"/tf/logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wavefile\n",
    "def readwav(file:str):\n",
    "    filepath = Path(file).absolute()\n",
    "    samplerate, data = wavfile.read((filepath))\n",
    "    # print(f\"samplerate = {samplerate}\")\n",
    "    return data,samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiming(data:np.ndarray,samplerate:int):\n",
    "    length = data.shape[0] / samplerate\n",
    "    return np.arange(0,length,1/samplerate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSignal(data:np.ndarray,t:np.ndarray,plot:bool=True):\n",
    "    ## normalize input\n",
    "    sig = data/np.amax(data)\n",
    "    sos = signal.butter(1, [.2,3], 'bp', fs=1000, output='sos')\n",
    "    filtered_heart = signal.sosfilt(sos, sig)\n",
    "    norm_heart = filtered_heart/np.amax(filtered_heart)\n",
    "    ## Removing noise\n",
    "    norm_heart = signal.signaltools.wiener(filtered_heart,300)\n",
    "    if plot:\n",
    "        _, (ax1, ax2,ax3) = plt.subplots(3, 1, sharex=True)\n",
    "        ax1.plot(t, sig)\n",
    "        ax1.set_title('Original Heart Rate Signal')\n",
    "        ax2.plot(t, norm_heart)\n",
    "        ax2.set_title('After Bandpass filter')\n",
    "        ax3.plot(t, norm_heart)\n",
    "        ax3.set_title('After Noise Filter')\n",
    "        ax3.set_xlabel('Time [seconds]')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return norm_heart\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSignal(file:str,plot:bool=False,loglevel:str=None):\n",
    "    data,samplerate = readwav(file)\n",
    "    t = getTiming(data,samplerate)\n",
    "    return t,filterSignal(data,t,plot),samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFolder = \"./heartbeats/classifications\"\n",
    "trainingpath = Path(trainingFolder)\n",
    "paths = [Path(dir[0]) for dir in walk(trainingpath)][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "plot = False\n",
    "def processFiles(indexedWave,classification,trainIndex):\n",
    "    wav,index = indexedWave\n",
    "    t,d,_ = generateSignal(wav,plot)\n",
    "    if(index < trainIndex):\n",
    "        train_data.append([d,t,classification])\n",
    "    else:\n",
    "        test_data.append([d,t,classification])\n",
    "\n",
    "\n",
    "def get_training_data(path:Path):   \n",
    "    classification = path.name\n",
    "    wavList = glob.glob(str(path.joinpath(\"*.wav\")))\n",
    "    trainIndex=int(math.ceil(len(wavList)*.8)) # use 80% of data for training\n",
    "    pool.map(lambda x: processFiles(x,classification,trainIndex),zip(wavList,range(0,len(wavList))) )\n",
    "    # for wav in zip(wavList,range(0,len(wavList))):\n",
    "    #     processFiles(wav,classification,trainIndex)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3737/2382820426.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_data=np.array(train_data)\n",
      "/tmp/ipykernel_3737/2382820426.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_data=np.array(test_data)\n"
     ]
    }
   ],
   "source": [
    "# Load Data \n",
    "#Loading data from this many files is intensive, speeding up w/ multithreading\n",
    "\n",
    "for path in paths:\n",
    "    get_training_data(path)\n",
    "train_data=np.array(train_data)\n",
    "test_data=np.array(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padData(series:pd.Series):\n",
    "    maxLength=0\n",
    "    for _,value in series.items():\n",
    "        maxLength=max(maxLength,len(value))\n",
    "    # for index,value in series.items():\n",
    "    # print(series)\n",
    "    # series[index]=sequence.pad_sequences(series[index],maxlen=maxLength,dtype='float64')\n",
    "    # print(maxLength)\n",
    "    # print(series)\n",
    "    mod = sequence.pad_sequences(series,maxlen=maxLength,padding='post',dtype='float64',value=0).tolist()\n",
    "    # print(mod)\n",
    "    return mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396900\n",
      "0      [-1.7519662909827494e-07, -1.175316157382523e-...\n",
      "1      [0.012417078213215934, 0.012940551628650217, 0...\n",
      "2      [0.0026388929361166207, 0.002623511752097469, ...\n",
      "3      [-6.868738632576114e-05, -5.080191118800028e-0...\n",
      "4      [4.827771064303697e-05, 4.891067453619705e-05,...\n",
      "                             ...                        \n",
      "138    [0.009117810768986333, 0.009172225133000471, 0...\n",
      "139    [-0.0017034451589284143, -0.001563755232277169...\n",
      "140    [0.002309420742175835, 0.002777685485082106, 0...\n",
      "141    [-0.0023368859321800145, -0.002342523847503692...\n",
      "142    [0.0077366706612047795, 0.007799375712485868, ...\n",
      "Name: signal, Length: 143, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.DataFrame(train_data,columns=[\"signal\",\"timing\",\"classification\"])\n",
    "train_set = pd.get_dummies(train_set,columns=[\"classification\"])\n",
    "test_set = pd.DataFrame(test_data,columns=[\"signal\",\"timing\",\"classification\"])\n",
    "test_set = pd.get_dummies(test_set,columns=[\"classification\"])\n",
    "# print(np.stack(padData(train_set[\"signal\"])))\n",
    "# print(train_set[\"signal\"])\n",
    "train_set[\"signal\"] = (padData(train_set[\"signal\"]))\n",
    "train_set[\"timing\"] = (padData(train_set[\"timing\"]))\n",
    "test_set[\"signal\"] = (padData(test_set[\"signal\"]))\n",
    "test_set[\"timing\"] = (padData(test_set[\"timing\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 2)\n",
      "[[-1.75196629e-07  0.00000000e+00]\n",
      " [ 1.24170782e-02  0.00000000e+00]\n",
      " [ 2.63889294e-03  0.00000000e+00]\n",
      " [-6.86873863e-05  0.00000000e+00]\n",
      " [ 4.82777106e-05  0.00000000e+00]\n",
      " [-3.63207646e-05  0.00000000e+00]\n",
      " [-3.65722487e-05  0.00000000e+00]\n",
      " [-1.43582201e-02  0.00000000e+00]\n",
      " [ 1.60950306e-06  0.00000000e+00]\n",
      " [-2.84769690e-02  0.00000000e+00]\n",
      " [-1.80348707e-06  0.00000000e+00]\n",
      " [ 2.55965482e-03  0.00000000e+00]\n",
      " [-5.97241935e-03  0.00000000e+00]\n",
      " [-3.00896645e-02  0.00000000e+00]\n",
      " [ 1.32214766e-02  0.00000000e+00]\n",
      " [-1.43365779e-04  0.00000000e+00]\n",
      " [ 1.48870399e-02  0.00000000e+00]\n",
      " [ 4.66192978e-04  0.00000000e+00]\n",
      " [ 2.79976048e-03  0.00000000e+00]\n",
      " [-5.57533107e-06  0.00000000e+00]\n",
      " [-3.90992126e-03  0.00000000e+00]\n",
      " [-1.07282432e-03  0.00000000e+00]\n",
      " [ 1.61530256e-03  0.00000000e+00]\n",
      " [ 1.27494134e-02  0.00000000e+00]\n",
      " [-7.86083549e-05  0.00000000e+00]\n",
      " [ 2.47518553e-04  0.00000000e+00]\n",
      " [-4.51966565e-06  0.00000000e+00]\n",
      " [ 1.46019884e-04  0.00000000e+00]\n",
      " [ 2.08741306e-03  0.00000000e+00]\n",
      " [ 1.95862695e-04  0.00000000e+00]\n",
      " [ 1.05811998e-05  0.00000000e+00]\n",
      " [-4.88538808e-04  0.00000000e+00]\n",
      " [ 9.32244291e-07  0.00000000e+00]\n",
      " [ 1.60062213e-03  0.00000000e+00]\n",
      " [ 8.25006474e-03  0.00000000e+00]\n",
      " [ 5.88041526e-05  0.00000000e+00]\n",
      " [ 4.02652954e-04  0.00000000e+00]\n",
      " [-5.06666979e-06  0.00000000e+00]\n",
      " [-1.50872217e-03  0.00000000e+00]\n",
      " [-3.39177475e-07  0.00000000e+00]\n",
      " [ 2.28439367e-02  0.00000000e+00]\n",
      " [-7.04992992e-03  0.00000000e+00]\n",
      " [ 9.20501157e-04  0.00000000e+00]\n",
      " [-4.32484650e-02  0.00000000e+00]\n",
      " [-7.63730223e-04  0.00000000e+00]\n",
      " [-2.24358217e-03  0.00000000e+00]\n",
      " [-1.03079577e-04  0.00000000e+00]\n",
      " [ 1.52390768e-02  0.00000000e+00]\n",
      " [-7.54148352e-03  0.00000000e+00]\n",
      " [-2.69483739e-02  0.00000000e+00]\n",
      " [-1.16189270e-05  0.00000000e+00]\n",
      " [-1.04560612e-02  0.00000000e+00]\n",
      " [-2.43348552e-02  0.00000000e+00]\n",
      " [-2.27285351e-02  0.00000000e+00]\n",
      " [-2.60126939e-03  0.00000000e+00]\n",
      " [ 3.65815366e-02  0.00000000e+00]\n",
      " [-1.02896697e-03  0.00000000e+00]\n",
      " [ 2.60078073e-03  0.00000000e+00]\n",
      " [ 2.07533291e-04  0.00000000e+00]\n",
      " [ 1.49229190e-03  0.00000000e+00]\n",
      " [ 2.68826934e-05  0.00000000e+00]\n",
      " [ 1.79820260e-03  0.00000000e+00]\n",
      " [-6.64437381e-07  0.00000000e+00]\n",
      " [-5.36502747e-04  0.00000000e+00]\n",
      " [-1.19690446e-02  0.00000000e+00]\n",
      " [ 2.79786623e-04  0.00000000e+00]\n",
      " [-2.52145026e-02  0.00000000e+00]\n",
      " [-5.11483623e-03  0.00000000e+00]\n",
      " [ 2.09921499e-03  0.00000000e+00]\n",
      " [-6.66811932e-04  0.00000000e+00]\n",
      " [ 2.26449389e-04  0.00000000e+00]\n",
      " [ 2.85219041e-03  0.00000000e+00]\n",
      " [-3.97878791e-05  0.00000000e+00]\n",
      " [ 1.49290545e-03  0.00000000e+00]\n",
      " [-1.14489920e-03  0.00000000e+00]\n",
      " [ 2.80091171e-03  0.00000000e+00]\n",
      " [ 1.29509829e-03  0.00000000e+00]\n",
      " [-1.84029248e-03  0.00000000e+00]\n",
      " [ 2.30371563e-02  0.00000000e+00]\n",
      " [ 4.78628494e-03  0.00000000e+00]\n",
      " [-1.50121064e-03  0.00000000e+00]\n",
      " [-4.88522584e-03  0.00000000e+00]\n",
      " [ 5.13535460e-02  0.00000000e+00]\n",
      " [-1.57203130e-02  0.00000000e+00]\n",
      " [ 4.52517250e-03  0.00000000e+00]\n",
      " [-3.52688430e-03  0.00000000e+00]\n",
      " [-6.27012607e-03  0.00000000e+00]\n",
      " [ 9.59080092e-03  0.00000000e+00]\n",
      " [-3.43278785e-02  0.00000000e+00]\n",
      " [ 2.14279928e-02  0.00000000e+00]\n",
      " [-3.53004632e-02  0.00000000e+00]\n",
      " [ 3.79559853e-02  0.00000000e+00]\n",
      " [-1.08740387e-02  0.00000000e+00]\n",
      " [-3.96040978e-02  0.00000000e+00]\n",
      " [-1.43606528e-02  0.00000000e+00]\n",
      " [-4.81098184e-03  0.00000000e+00]\n",
      " [-1.62335260e-02  0.00000000e+00]\n",
      " [-3.82263657e-03  0.00000000e+00]\n",
      " [-2.95165670e-02  0.00000000e+00]\n",
      " [-9.25477961e-04  0.00000000e+00]\n",
      " [-1.27668341e-02  0.00000000e+00]\n",
      " [-2.39411706e-02  0.00000000e+00]\n",
      " [ 7.11261424e-03  0.00000000e+00]\n",
      " [-1.35398177e-03  0.00000000e+00]\n",
      " [ 1.79331850e-04  0.00000000e+00]\n",
      " [ 3.77745809e-03  0.00000000e+00]\n",
      " [-1.94623699e-03  0.00000000e+00]\n",
      " [-2.44506161e-03  0.00000000e+00]\n",
      " [-8.11673017e-03  0.00000000e+00]\n",
      " [-3.86668202e-04  0.00000000e+00]\n",
      " [-7.46176775e-03  0.00000000e+00]\n",
      " [-2.33318485e-05  0.00000000e+00]\n",
      " [ 6.15205918e-04  0.00000000e+00]\n",
      " [-3.57965130e-03  0.00000000e+00]\n",
      " [-1.67596192e-03  0.00000000e+00]\n",
      " [ 9.05514931e-06  0.00000000e+00]\n",
      " [ 1.91857228e-02  0.00000000e+00]\n",
      " [ 2.77063264e-05  0.00000000e+00]\n",
      " [ 1.25515716e-04  0.00000000e+00]\n",
      " [-9.26809504e-04  0.00000000e+00]\n",
      " [-1.22305273e-03  0.00000000e+00]\n",
      " [-1.08856319e-03  0.00000000e+00]\n",
      " [ 2.61981462e-02  0.00000000e+00]\n",
      " [-4.31944059e-05  0.00000000e+00]\n",
      " [ 1.15060847e-02  0.00000000e+00]\n",
      " [ 9.97703386e-04  0.00000000e+00]\n",
      " [ 5.59600680e-03  0.00000000e+00]\n",
      " [ 1.70524795e-03  0.00000000e+00]\n",
      " [-1.05547314e-03  0.00000000e+00]\n",
      " [-3.71684294e-07  0.00000000e+00]\n",
      " [-7.72852183e-03  0.00000000e+00]\n",
      " [-1.43231320e-08  0.00000000e+00]\n",
      " [-6.49617845e-04  0.00000000e+00]\n",
      " [ 3.25557768e-04  0.00000000e+00]\n",
      " [-5.10181355e-03  0.00000000e+00]\n",
      " [ 1.65656344e-04  0.00000000e+00]\n",
      " [-6.72065989e-03  0.00000000e+00]\n",
      " [-8.49796569e-08  0.00000000e+00]\n",
      " [ 9.11781077e-03  0.00000000e+00]\n",
      " [-1.70344516e-03  0.00000000e+00]\n",
      " [ 2.30942074e-03  0.00000000e+00]\n",
      " [-2.33688593e-03  0.00000000e+00]\n",
      " [ 7.73667066e-03  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train =  train_set[[\"signal\",\"timing\"]].to_numpy()\n",
    "y_train = train_set.iloc[:,2:].to_numpy()\n",
    "# x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
    "\n",
    "\n",
    "x_test =  test_set[[\"signal\",\"timing\"]].to_numpy()\n",
    "y_test = test_set.iloc[:,2:].to_numpy()\n",
    "# x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n",
    "print(np.shape(x_train))\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(16, input_shape=(x_train.shape[1],1), activation=\"relu\"))\n",
    "model.add(Dense(8, activation='tanh'))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=512, kernel_size=1000, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(LSTM(400,return_sequences=False))\n",
    "\n",
    "# # model.add(LSTM(400,return_sequences=False,recurrent_activation='selu',kernel_initializer='lecun_normal', input_shape=(x_train.shape[1],1)))\n",
    "\n",
    "# model.add(Dense(5, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1664, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 4994, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 2, 5) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3737/500024442.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 809, in train_step\n        loss = self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1664, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 4994, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 5) and (None, 2, 5) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=512)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3737/2766529932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.30%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
