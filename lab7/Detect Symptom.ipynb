{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from os import walk\n",
    "from keras.preprocessing import sequence\n",
    "from scipy.io import wavfile\n",
    "import glob\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import keras\n",
    "pool=ThreadPool(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/tf/logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "#%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wavefile\n",
    "def readwav(file:str):\n",
    "    filepath = Path(file).absolute()\n",
    "    samplerate, data = wavfile.read((filepath))\n",
    "    # print(f\"samplerate = {samplerate}\")\n",
    "    return data,samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiming(data:np.ndarray,samplerate:int):\n",
    "    length = data.shape[0] / samplerate\n",
    "    return np.arange(0,length,1/samplerate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterSignal(data:np.ndarray,t:np.ndarray,plot:bool=True,length=None,filter=True):\n",
    "    ## normalize input\n",
    "    sig = data/np.amax(data)\n",
    "    norm_heart = data/np.amax(data)\n",
    "    sos = signal.butter(1, [.2,195], 'bp', fs=1000, output='sos')\n",
    "    filtered_heart = signal.sosfilt(sos, sig)\n",
    "    ## Removing noise\n",
    "    noise_heart = signal.signaltools.wiener(filtered_heart,300)\n",
    "    noise_heart = filtered_heart\n",
    "    if(not filter):\n",
    "        noise_heart = norm_heart\n",
    "    if length:\n",
    "        resampled,resampledt = signal.resample(noise_heart,33075,t=t)\n",
    "    if plot:\n",
    "        if length:\n",
    "            # _, (ax1, ax2,ax3,ax4) = plt.subplots(4, 1, sharex=True)\n",
    "            # ax4.plot(resampledt, resampled)\n",
    "            # ax4.set_title('After Resampling')\n",
    "            # ax4.set_xlabel('Time [seconds]')\n",
    "            _, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "            ax1.plot(t, sig)\n",
    "            ax1.set_title('Original Heart Rate Signal')\n",
    "            ax2.plot(resampledt, resampled)\n",
    "            ax2.set_title('After Resampling')\n",
    "            ax2.set_xlabel('Time [seconds]')\n",
    "        \n",
    "        #     _, (ax1, ax2,ax3) = plt.subplots(3, 1, sharex=True)\n",
    "        # ax1.plot(t, sig)\n",
    "        # ax1.set_title('Original Heart Rate Signal')\n",
    "        # ax2.plot(t, norm_heart)\n",
    "        # ax2.set_title('After Bandpass filter')\n",
    "        # ax3.plot(t, noise_heart)\n",
    "        # ax3.set_title('After Noise Filter')\n",
    "        # ax3.set_xlabel('Time [seconds]')\n",
    "\n",
    "        # plt.tight_layout()\n",
    "    plt.show()\n",
    "    return (resampled,resampledt) if length else (noise_heart,t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "def generateSignal(file:str,plot:bool=False,loglevel:str=None):\n",
    "    data,samplerate = readwav(file)\n",
    "    length = data.shape[0] / samplerate\n",
    "    lengths.append(length)\n",
    "    t = getTiming(data,samplerate)\n",
    "\n",
    "    sig,t = filterSignal(data,t,plot,length)\n",
    "    return t,sig,samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingFolder = \"./heartbeats/classifications\"\n",
    "trainingpath = Path(trainingFolder)\n",
    "paths = [Path(dir[0]) for dir in walk(trainingpath)][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "# test_data = []\n",
    "plot = False\n",
    "def processFiles(indexedWave,classification,trainIndex):\n",
    "    wav,index = indexedWave\n",
    "\n",
    "    t,d,_ = generateSignal(wav,plot)\n",
    "    # if(index < trainIndex):\n",
    "    train_data.append([d,t,classification])\n",
    "    # else:\n",
    "        # test_data.append([d,t,classification])\n",
    "\n",
    "\n",
    "def get_training_data(path:Path):   \n",
    "    classification = path.name\n",
    "    wavList = glob.glob(str(path.joinpath(\"*.wav\")))\n",
    "    trainIndex=int(math.ceil(len(wavList)*.8)) # use 80% of data for training\n",
    "    pool.map(lambda x: processFiles(x,classification,trainIndex),zip(wavList,range(0,len(wavList))) )\n",
    "    # for wav in zip(wavList,range(0,len(wavList))):\n",
    "    #     processFiles(wav,classification,trainIndex)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data \n",
    "#Loading data from this many files is intensive, speeding up w/ multithreading\n",
    "\n",
    "for path in paths:\n",
    "    get_training_data(path)\n",
    "#train_data=np.array(train_data)\n",
    "#test_data=np.array(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig,time,classification = zip(*train_data)\n",
    "getMaxLength = lambda list: max([len(item) for item in list])\n",
    "max_length = getMaxLength(sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_data,columns=[\"bpm\",\"ibi\",\"sdnn\",\"sdsd\",\"rmssd\",\"pnn20\",\"pnn50\",\"hr_mad\",\"sd1\",\"sd2\",\"s\",\"sdr\",\"breathingrate\",\"classification\"])\n",
    "# print(df[[\"signal\",\"time\"]].values)\n",
    "\n",
    "# y = df.iloc\n",
    "# sequence.pad_sequences(df[[\"signal\",\"time\"]].values,maxlen=max_length,dtype=\"float64\")\n",
    "# print(df.iloc[2])\n",
    "df = pd.get_dummies(df,columns=[\"classification\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 33075, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.iloc[:,2:].values\n",
    "x = np.dstack((xs,xt))\n",
    "# x = xs\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 15, 1, 2205, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "n_steps, n_length = 15, 2205\n",
    "x = x.reshape((x.shape[0], n_steps,1, n_length, 2))\n",
    "# x = x.reshape((x.shape[0], n_steps, n_length, 2))\n",
    "\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 15, 1, 2205, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "# y=tf.keras.utils.to_categorical(np.array(y),num_classes=5)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv_lstm2d (ConvLSTM2D)    (None, 1, 2203, 32)       13184     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1, 2203, 32)      128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 1, 1101, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 1101, 32)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 1, 1099, 32)       3104      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1, 1099, 32)      128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 1097, 32)       3104      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 1, 548, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 548, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 546, 32)        3104      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1, 546, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 1, 544, 32)        3104      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 1, 272, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 272, 32)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8704)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1114240   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,140,869\n",
      "Trainable params: 1,140,677\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "model = Sequential()\n",
    "model.add(layers.ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "# model.add(layers.Conv2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "model.add(layers.MaxPooling2D((1, 2)))\n",
    "model.add(layers.Dropout(.2))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "model.add(layers.MaxPooling2D((1, 2)))\n",
    "model.add(layers.Dropout(.2))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "model.add(layers.MaxPooling2D((1, 2)))\n",
    "model.add(layers.Dropout(.2))\n",
    "# model.add(Conv1D(filters=16, kernel_size=4, activation='relu'))\n",
    "\n",
    "# model.add(ConvLSTM2D(filters=16, kernel_size=(1,3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "model.build(np.shape(x_train))\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define model\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "#     model = Sequential()\n",
    "#     # model.add(ConvLSTM2D(filters=32, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, 2)))\n",
    "#     model.add(Conv1D(filters=16, kernel_size=4, activation='relu'))\n",
    "#     model.add(LSTM(100))\n",
    "\n",
    "#     # # model.add(ConvLSTM2D(filters=16, kernel_size=(1,3), activation='relu'))\n",
    "\n",
    "#     # model.add(Dropout(0.5))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(5, activation='softmax'))\n",
    "#     model.build(np.shape(x_train))\n",
    "#     print(model.summary())\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "158/158 [==============================] - 18s 99ms/step - loss: 2.9080 - accuracy: 0.2468\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 1.6113 - accuracy: 0.2595\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 1.4766 - accuracy: 0.3354\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 15s 97ms/step - loss: 1.3156 - accuracy: 0.4367\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 15s 95ms/step - loss: 1.0768 - accuracy: 0.5380\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 14s 91ms/step - loss: 0.8718 - accuracy: 0.6646\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 14s 90ms/step - loss: 0.7018 - accuracy: 0.7595\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 15s 92ms/step - loss: 0.5443 - accuracy: 0.8291\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 15s 95ms/step - loss: 0.3441 - accuracy: 0.8671\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 15s 93ms/step - loss: 0.3372 - accuracy: 0.8608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01fc1f77f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.008731842041016, 0.3333333432674408]\n",
      "Accuracy: 6.171% (+/-5.838)\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(scores)\n",
    "m, s = np.mean(scores), np.std(scores)\n",
    "print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
